{% assign headings = headings | push: "Hands-On Activities" %}

{%- assign my-event = site.data.bsswt[page.event-label].event -%}
{%- assign my-artifacts = site.data.bsswt[page.event-label].artifacts -%}
{%- include key-artifact-shorthands artifacts=my-artifacts -%}

### Introduction

The hands-on activity for this {{ my-event.title-type | default: "event" }} involves the use of a large language model (LLM) to generate tests and code according to specifications (prompts) you will develop.  Participation in these activities is encouraged, but not required.  After interested participants have been given some time to try the exercise on their own, the instructor will review their prompts and the resulting code with the class and these materials will be made available to all participants.

You can participate in the hands-on section in two modes: using the LLM's **web interface**, or using **CodeScribe**, a tool that enables using chat-completion through the API interface of the LLM. The main objectives of the hands-on can be met by using the web interface. The advantage of using CodeScribe is to get exposure to the chat-completion technique, and getting to know a tool that can be very handy for writing code.

The instructor will work in the C language, but a surface understanding of the language will suffice to follow the explanations. For your own work, you can prompt the LLM to generate code in the language of your choice. Evaluation of the generated code (and revising the prompt accordingly) will be part of the hands-on activity.  For the purposes of the tutorial, inspection of the code will suffice to gauge its appropriateness, but if you wish to more rigorously validate the generated code, you will need to be able to access an environment to build and run it either locally or remotely.

### Advance preparation

If you wish to participate in the hands-on activities, we strongly encourage you to do a bit of preparation **before you leave for St. Louis**.  This is especially true if you want to use CodeScribe, which may require some advance interaction with the tool developers to integrate new LLM APIs.

#### Preparation for using the LLM **web interface**

1. You will need access to an LLM chat tool.  The instructor will be using [ChatGPT](https://chatgpt.com/), but any comparable LLM, including institutionally-supported tools, should work.

2. (OPTIONAL) If you wish to build and run the code generated by the LLM, you will need access (local or remote) to an appropriate environment.  As stated above, you can use whatever language you prefer for your hands-on activity.  The instructor will work in C.

#### Preparation for using **CodeScribe**

*It is important that you do this preparation with enough lead time that we can assist you if necessary **before SC25 starts**.  **We will not be able to provide any support for CodeScribe setup issues during the tutorial itself**.*

1. You will need API access to an LLM tool. This is a level beyond the web interface and it incurs an extra charge on some platforms. However many institutionally-supported LLMs offer API access at no additional cost.  You will be responsible for any additional costs incurred. The instructor will be using [ChatGPT](https://chatgpt.com/), but any comparable LLM should work.  CodeScribe also supports several freely downloadable models which can be run locally:

    - <https://huggingface.co/google/gemma-2-2b-it>
    - <https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct>
    - <https://huggingface.co/codellama/CodeLlama-7b-Instruct-hf/tree/main>

    The first two models are smaller and may be faster on laptops. Download instructions can be found here: <https://huggingface.co/docs/hub/en/models-downloading>.  Note that if you're downloading Hugging Face models using their CLI, they will be downloaded to their cache folder (`~/.cache/huggingface/hub/` or your operating system's equivalent).  If you use `git clone` you can put the models wherever you want.

2. CodeScribe is a Python code, so you will need a working Python installation on a system that you will be able to access (local or remote) during the tutorial.

3. Download and install CodeScribe from <https://github.com/adubey64/CodeScribe>
  a. Installation instructions are provided in the README file: <https://github.com/adubey64/CodeScribe?tab=readme-ov-file#installation>
  b. You are encouraged to watch the two tutorials on the installation and use of CodeScribe in this Box folder: <https://anl.app.box.com/folder/336154643880?s=zv3zdbphqprdz8rjh1c84xpeqd8yg32u>.  They are 19 minutes and 11 minutes long, respectively. The tutorials were prepared before "generate" command was added to the tool, so there is no mention of it in either tutorial, but it works like the "translate" command.

4. You will need to integrate your CodeScribe installation with the API of your LLM of choice.  Basic instructions are provided in the README file: <https://github.com/adubey64/CodeScribe?tab=readme-ov-file#integrating-llm-of-choice>. It may be necessary for you to add support for your particular model to CodeScribe. You should look at the file <https://github.com/adubey64/CodeScribe/blob/development/code_scribe/lib/_llm.py>, copy the class most closely resembling the target model and create a pull request in our repository. With enough lead time we will try our best to help make it work.

5. (OPTIONAL) If you wish to build and run the code generated by the LLM, you will need access (local or remote) to an appropriate environment.  As stated above, you can use whatever language you prefer for your hands-on activity.  The instructor will work in C.

### During the tutorial

During the tutorial, we will make available the prompts the instructor used and the generated code for your reference.  Links will be provided here.
